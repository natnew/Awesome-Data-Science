{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/natnew/Awesome-Data-Science/blob/main/notebooks/02_rag_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "# Hello World: RAG (Retrieval Augmented Generation) ðŸ¤–\n",
                "\n",
                "This notebook demonstrates the core concepts of **RAG**â€”the technique used to let LLMs \"chat with your data\".\n",
                "\n",
                "We will build a simple RAG pipeline from scratch (no complex libraries required for this demo) to understand how it works under the hood."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1: The \"Knowledge Base\"\n",
                "In a real app, this would be a Vector Database (like Pinecone or Chroma). Here, we'll use a simple Python dictionary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Our tiny knowledge base\n",
                "knowledge_base = {\n",
                "    \"what is rag\": \"RAG stands for Retrieval-Augmented Generation. It combines an LLM with a retrieval system.\",\n",
                "    \"who created python\": \"Python was created by Guido van Rossum and released in 1991.\",\n",
                "    \"what is better pandas or polars\": \"Polars is generally faster for large datasets due to Rust implementation, but Pandas has a larger ecosystem.\"\n",
                "}\n",
                "\n",
                "print(\"Knowledge base loaded with\", len(knowledge_base), \"facts.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 2: The \"Retriever\"\n",
                "The retriever's job is to find the most relevant piece of information for a user's query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simple_retriever(query):\n",
                "    # A naive keyword search usually performed by embeddings\n",
                "    key_match = None\n",
                "    for key in knowledge_base:\n",
                "        if key in query.lower():\n",
                "            return knowledge_base[key]\n",
                "    return None\n",
                "\n",
                "query = \"Who created Python?\"\n",
                "context = simple_retriever(query)\n",
                "\n",
                "print(f\"Query: {query}\")\n",
                "print(f\"Retrieved Context: {context}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: The \"Generator\" (Mock LLM)\n",
                "Now we augment the prompt with the context and send it to the LLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mock_llm_generate(prompt, context):\n",
                "    if context:\n",
                "        return f\"[AI Answer based on context]: {context}\"\n",
                "    else:\n",
                "        return \"[AI Answer]: I don't know that based on my knowledge base.\"\n",
                "\n",
                "# The full RAG flow\n",
                "full_prompt = f\"Answer this: {query} \\n using this info: {context}\"\n",
                "response = mock_llm_generate(query, context)\n",
                "\n",
                "print(\"Final RAG Response:\", response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Conclusion\n",
                "You just built the logic for RAG!\n",
                "\n",
                "**In the real world:**\n",
                "- **Dictionary** -> Vector Database (Chroma/Pinecone)\n",
                "- **Keyword Search** -> Cosine Similarity of Embeddings\n",
                "- **Mock Function** -> OpenAI/Anthropic API call"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}