{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/natnew/Awesome-Data-Science/blob/main/notebooks/03_next_gen_model_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Next-Gen Model Prompting for Data Science ðŸš€\n",
    "\n",
    "This notebook demonstrates effective prompting strategies for the latest generation of language models:\n",
    "- **Opus 4.6** - Advanced reasoning for complex analysis\n",
    "- **Sonnet 4.6** - Balanced performance for production workflows\n",
    "- **Qwen 2.5** - Open-source powerhouse for diverse tasks\n",
    "- **GLM-5** - Bilingual capabilities with strong coding skills\n",
    "\n",
    "We'll showcase prompts tailored to common **data science and ML engineering tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Dataset Analysis & Exploration](#1-dataset-analysis--exploration)\n",
    "2. [Metrics Design & Dashboard Planning](#2-metrics-design--dashboard-planning)\n",
    "3. [Fraud Detection & Risk Analytics](#3-fraud-detection--risk-analytics)\n",
    "4. [Human-in-the-Loop System Design](#4-human-in-the-loop-system-design)\n",
    "5. [Dataset Collection & Curation](#5-dataset-collection--curation)\n",
    "6. [AI Infrastructure Architecture](#6-ai-infrastructure-architecture)\n",
    "7. [LLM Evaluation Frameworks](#7-llm-evaluation-frameworks)\n",
    "8. [Model-Specific Strengths](#8-model-specific-strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Dataset Analysis & Exploration\n",
    "\n",
    "### Best Model: **Opus 4.6** (for deep analysis) or **Qwen 2.5** (for speed)\n",
    "\n",
    "These models excel at understanding complex data patterns and providing actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt Template for Dataset Analysis\n",
    "\n",
    "prompt_dataset_analysis = \"\"\"\n",
    "You are a senior data scientist analyzing a customer transaction dataset.\n",
    "\n",
    "Dataset Schema:\n",
    "- transaction_id (string)\n",
    "- user_id (string)\n",
    "- amount (float)\n",
    "- timestamp (datetime)\n",
    "- category (string: ['retail', 'online', 'subscription'])\n",
    "- is_fraudulent (boolean)\n",
    "\n",
    "Sample Statistics:\n",
    "- Total records: 1.2M\n",
    "- Fraud rate: 2.3%\n",
    "- Avg transaction amount: $156.78\n",
    "- Date range: 2023-01-01 to 2024-12-31\n",
    "\n",
    "Tasks:\n",
    "1. Identify 3 potential data quality issues to investigate\n",
    "2. Suggest 5 exploratory visualizations that would reveal insights\n",
    "3. Recommend a hypothesis testing approach for fraud patterns\n",
    "4. Propose feature engineering ideas for a fraud detection model\n",
    "\n",
    "Format your response with clear sections and prioritize actionable recommendations.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“Š Dataset Analysis Prompt Template:\")\n",
    "print(prompt_dataset_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Prompting Tips for Dataset Analysis:\n",
    "- **Be specific** about schema, statistics, and business context\n",
    "- **Ask for structured output** (numbered lists, sections)\n",
    "- **Request prioritization** when multiple options exist\n",
    "- **Include sample data** when possible for better understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Metrics Design & Dashboard Planning\n",
    "\n",
    "### Best Model: **Sonnet 4.6** (balanced speed + quality) or **GLM-5** (for technical depth)\n",
    "\n",
    "These models are excellent at translating business objectives into measurable KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt for Metrics Design\n",
    "\n",
    "prompt_metrics_design = \"\"\"\n",
    "Design a comprehensive metrics framework for a risk management platform.\n",
    "\n",
    "Business Context:\n",
    "- B2B SaaS platform for fraud prevention\n",
    "- Serves 500+ enterprise clients\n",
    "- Processes 10M+ transactions/day\n",
    "- Key business goals: Reduce fraud losses by 30%, Increase approval rates by 15%\n",
    "\n",
    "Requirements:\n",
    "1. Define 4-6 North Star metrics aligned with business goals\n",
    "2. For each metric, specify:\n",
    "   - Formula/calculation method\n",
    "   - Data sources required\n",
    "   - Expected update frequency\n",
    "   - Target threshold or benchmark\n",
    "3. Suggest a dashboard layout with 3 sections: Real-time, Daily, Strategic\n",
    "4. Identify leading vs lagging indicators\n",
    "5. Recommend alerting thresholds for critical metrics\n",
    "\n",
    "Output format: Create a table for metrics and a mockup description for the dashboard.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ˆ Metrics Design Prompt Template:\")\n",
    "print(prompt_metrics_design)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Prompting Technique: Chain-of-Thought for Metrics\n",
    "\n",
    "For complex metric design, use **step-by-step reasoning**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_metrics_cot = \"\"\"\n",
    "Let's design a fraud detection effectiveness metric step by step:\n",
    "\n",
    "Step 1: What are we trying to measure? (Think about both false positives and false negatives)\n",
    "Step 2: What data do we have available? (List the tables and fields)\n",
    "Step 3: What formula would capture both precision and recall?\n",
    "Step 4: How should we weight false positives vs false negatives given business impact?\n",
    "Step 5: Propose the final metric formula with justification.\n",
    "\n",
    "Walk through each step methodically.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ”— Chain-of-Thought Metrics Prompt:\")\n",
    "print(prompt_metrics_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Fraud Detection & Risk Analytics\n",
    "\n",
    "### Best Model: **Opus 4.6** (complex patterns) or **Qwen 2.5** (fast iteration)\n",
    "\n",
    "Fraud detection requires understanding subtle patterns and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt for Fraud Analysis\n",
    "\n",
    "prompt_fraud_analysis = \"\"\"\n",
    "You are a fraud analytics expert reviewing transaction patterns.\n",
    "\n",
    "Scenario:\n",
    "We've noticed a 40% spike in chargebacks from transactions in the $50-$100 range, \n",
    "primarily occurring between 2-4 AM EST, with a concentration in the gaming and \n",
    "digital goods categories.\n",
    "\n",
    "Historical Context:\n",
    "- Normal chargeback rate: 0.8%\n",
    "- Current rate in affected segment: 3.2%\n",
    "- Time period: Last 14 days\n",
    "- Affected merchants: 12 out of 450\n",
    "\n",
    "Analysis Request:\n",
    "1. Hypothesize 3 potential root causes (rank by likelihood)\n",
    "2. Design an investigation plan with specific queries/analyses\n",
    "3. Propose immediate mitigation strategies (short-term)\n",
    "4. Suggest long-term preventive measures\n",
    "5. Recommend additional data points to collect\n",
    "\n",
    "Consider both technical fraud vectors and legitimate user behavior anomalies.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ” Fraud Analysis Prompt Template:\")\n",
    "print(prompt_fraud_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Specific Approach:\n",
    "\n",
    "**For Opus 4.6:** Add \"Think like a red team - what would a sophisticated fraudster do?\"  \n",
    "**For Qwen 2.5:** Include \"Provide code snippets for SQL queries to investigate each hypothesis\"  \n",
    "**For GLM-5:** Leverage bilingual capability with \"Explain in both English and Chinese for global teams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Human-in-the-Loop System Design\n",
    "\n",
    "### Best Model: **Sonnet 4.6** (balanced reasoning) or **GLM-5** (system architecture)\n",
    "\n",
    "HITL systems require careful balance between automation and human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt for HITL System Design\n",
    "\n",
    "prompt_hitl_design = \"\"\"\n",
    "Design a human-in-the-loop system for content moderation.\n",
    "\n",
    "System Requirements:\n",
    "- Initial ML model: 92% accuracy on test set\n",
    "- Volume: 100K items/day\n",
    "- Human reviewer capacity: 5,000 items/day\n",
    "- Categories: Safe, Borderline, Unsafe, Illegal\n",
    "- Latency requirement: 95% of items decided within 2 hours\n",
    "\n",
    "Design Tasks:\n",
    "1. Define routing logic: Which items go to humans vs auto-decision?\n",
    "2. Specify confidence thresholds for each category\n",
    "3. Design the reviewer interface (what info to show, what actions available)\n",
    "4. Create a feedback loop: How do human decisions improve the model?\n",
    "5. Define escalation rules for edge cases\n",
    "6. Propose quality assurance mechanism for human reviewers\n",
    "7. Estimate expected system performance (accuracy, latency, cost)\n",
    "\n",
    "Include a workflow diagram description and key implementation considerations.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ‘¥ HITL System Design Prompt:\")\n",
    "print(prompt_hitl_design)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pro Tip: Use Few-Shot Examples for Complex Design\n",
    "\n",
    "When designing systems, provide 1-2 analogous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_hitl_fewshot = \"\"\"\n",
    "Design a HITL system using these reference examples:\n",
    "\n",
    "Example 1 - Email Spam Filter:\n",
    "- Auto-allow: confidence > 95% (ham)\n",
    "- Auto-block: confidence > 99% (spam)\n",
    "- Human review: 95-99% confidence range\n",
    "- Result: Reduced false positives by 60%\n",
    "\n",
    "Example 2 - Loan Approval:\n",
    "- Auto-approve: credit score > 750 + income verification\n",
    "- Auto-reject: score < 600 + no collateral\n",
    "- Human review: Middle range + complex cases\n",
    "- Result: 10x faster decisions, maintained approval quality\n",
    "\n",
    "Now design a similar system for: [Your specific use case]\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ Few-Shot HITL Prompt:\")\n",
    "print(prompt_hitl_fewshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Dataset Collection & Curation\n",
    "\n",
    "### Best Model: **Qwen 2.5** (practical methodology) or **Opus 4.6** (research-grade rigor)\n",
    "\n",
    "Quality datasets are the foundation of ML success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt for Dataset Curation Strategy\n",
    "\n",
    "prompt_dataset_curation = \"\"\"\n",
    "Design a dataset collection and curation methodology for training a medical diagnosis assistant.\n",
    "\n",
    "Project Scope:\n",
    "- Domain: Radiology image analysis (X-rays, CT scans)\n",
    "- Target: 50K labeled images across 15 conditions\n",
    "- Quality bar: Must meet FDA-equivalent standards\n",
    "- Timeline: 6 months\n",
    "- Team: 3 radiologists, 2 ML engineers, 1 project manager\n",
    "\n",
    "Methodology Requirements:\n",
    "1. Data sourcing strategy:\n",
    "   - Identify 3-5 potential data sources\n",
    "   - Evaluate each for quality, volume, licensing, privacy compliance\n",
    "   - Recommend primary and backup sources\n",
    "\n",
    "2. Annotation protocol:\n",
    "   - Define labeling schema (what to annotate, format)\n",
    "   - Create quality control process (inter-rater agreement targets)\n",
    "   - Design reviewer training program\n",
    "   - Specify tools and infrastructure needed\n",
    "\n",
    "3. Bias mitigation:\n",
    "   - Identify 5 potential bias sources\n",
    "   - Propose fairness metrics to track\n",
    "   - Design sampling strategy to ensure representation\n",
    "\n",
    "4. Version control and documentation:\n",
    "   - Dataset versioning scheme\n",
    "   - Metadata to capture per image\n",
    "   - Audit trail requirements\n",
    "\n",
    "5. Risk assessment:\n",
    "   - List top 5 risks and mitigation plans\n",
    "\n",
    "Deliver a comprehensive 2-page project plan outline.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“š Dataset Curation Methodology Prompt:\")\n",
    "print(prompt_dataset_curation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique: Iterative Refinement\n",
    "\n",
    "For complex planning tasks, use a multi-turn approach:\n",
    "1. First turn: Get high-level strategy\n",
    "2. Second turn: \"Expand section 2 (Annotation Protocol) with specific examples\"\n",
    "3. Third turn: \"Review for HIPAA compliance gaps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. AI Infrastructure Architecture\n",
    "\n",
    "### Best Model: **GLM-5** (coding + architecture) or **Sonnet 4.6** (production systems)\n",
    "\n",
    "Scaling AI systems requires robust infrastructure design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt for ML Infrastructure Design\n",
    "\n",
    "prompt_ml_infra = \"\"\"\n",
    "Design a production ML infrastructure for real-time fraud detection.\n",
    "\n",
    "System Requirements:\n",
    "- Throughput: 10,000 predictions/second\n",
    "- Latency: p99 < 50ms\n",
    "- Model: XGBoost ensemble (5 models)\n",
    "- Features: 200 features (100 real-time, 100 from feature store)\n",
    "- Training frequency: Daily retraining\n",
    "- Deployment: Multi-region (US, EU, APAC)\n",
    "- Monitoring: Real-time drift detection\n",
    "\n",
    "Architecture Components to Design:\n",
    "\n",
    "1. **Model Serving**:\n",
    "   - Recommend serving framework (TensorFlow Serving, KServe, BentoML, etc.)\n",
    "   - Define scaling strategy (horizontal/vertical, autoscaling rules)\n",
    "   - Specify instance types and resource allocation\n",
    "\n",
    "2. **Feature Store**:\n",
    "   - Choose technology (Feast, Tecton, Hopsworks, custom)\n",
    "   - Design online vs offline feature architecture\n",
    "   - Cache strategy for hot features\n",
    "\n",
    "3. **Training Pipeline**:\n",
    "   - Orchestration tool (Airflow, Kubeflow, Prefect)\n",
    "   - Data versioning approach\n",
    "   - Experiment tracking (MLflow, W&B, Neptune)\n",
    "   - Model registry and versioning\n",
    "\n",
    "4. **Monitoring & Observability**:\n",
    "   - Metrics to track (latency, throughput, accuracy, drift)\n",
    "   - Alerting rules\n",
    "   - Logging strategy\n",
    "   - A/B testing framework\n",
    "\n",
    "5. **Deployment Strategy**:\n",
    "   - Blue-green vs canary deployment\n",
    "   - Rollback procedure\n",
    "   - Shadow mode testing\n",
    "\n",
    "6. **Cost Optimization**:\n",
    "   - Estimated monthly cost breakdown\n",
    "   - Cost reduction strategies\n",
    "\n",
    "Provide a diagram description, technology stack table, and implementation timeline.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ—ï¸ ML Infrastructure Design Prompt:\")\n",
    "print(prompt_ml_infra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-5 Advantage: Request Code Artifacts\n",
    "\n",
    "GLM-5 excels at generating implementation code alongside architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infra_with_code = \"\"\"\n",
    "Design the ML infrastructure AND provide starter code for:\n",
    "\n",
    "1. FastAPI endpoint for model serving\n",
    "2. Prometheus metrics collection\n",
    "3. Feature retrieval from Redis cache\n",
    "4. Model versioning with MLflow\n",
    "\n",
    "Use best practices: error handling, logging, type hints, async where appropriate.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ’» Infrastructure + Code Prompt (GLM-5):\")\n",
    "print(prompt_infra_with_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. LLM Evaluation Frameworks\n",
    "\n",
    "### Best Model: **Opus 4.6** (comprehensive evaluation) or **Qwen 2.5** (practical metrics)\n",
    "\n",
    "Evaluating LLMs requires going beyond accuracyâ€”considering performance, bias, safety, and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Prompt for LLM Evaluation Framework\n",
    "\n",
    "prompt_llm_eval = \"\"\"\n",
    "Design a comprehensive evaluation framework for a customer support chatbot powered by LLMs.\n",
    "\n",
    "Product Context:\n",
    "- Use case: E-commerce customer support (returns, shipping, product questions)\n",
    "- Volume: 50K conversations/month\n",
    "- Current baseline: Human agents with 85% CSAT, 5-minute avg resolution time\n",
    "- Candidate models: GPT-4o, Claude Sonnet 4.6, Llama 3.3 70B\n",
    "\n",
    "Evaluation Dimensions:\n",
    "\n",
    "1. **Performance Metrics**:\n",
    "   - Define task-specific accuracy measures\n",
    "   - Response quality rubric (helpfulness, correctness, tone)\n",
    "   - Latency benchmarks (p50, p95, p99)\n",
    "   - Cost per conversation\n",
    "\n",
    "2. **Reliability Assessment**:\n",
    "   - Consistency testing (same question, multiple runs)\n",
    "   - Edge case handling (nonsensical input, adversarial queries)\n",
    "   - Hallucination detection methodology\n",
    "   - Failure mode analysis\n",
    "\n",
    "3. **Bias & Safety**:\n",
    "   - Fairness metrics across customer demographics\n",
    "   - Toxicity/harm detection\n",
    "   - Personally identifiable information (PII) leakage tests\n",
    "   - Jailbreak resistance\n",
    "\n",
    "4. **Evaluation Dataset**:\n",
    "   - Composition: X% real conversations, Y% synthetic adversarial\n",
    "   - Size requirements for statistical significance\n",
    "   - Annotation scheme (who labels, inter-rater agreement)\n",
    "\n",
    "5. **Evaluation Pipeline**:\n",
    "   - Automated vs human evaluation split\n",
    "   - Tools to use (LLM-as-judge, human raters, rule-based)\n",
    "   - Continuous monitoring post-deployment\n",
    "\n",
    "6. **Decision Framework**:\n",
    "   - Minimum acceptable thresholds per metric\n",
    "   - How to trade off performance vs cost\n",
    "   - Go/no-go criteria\n",
    "\n",
    "Provide:\n",
    "- Evaluation metric table with formulas\n",
    "- Sample test cases (5 examples)\n",
    "- Recommended tooling\n",
    "- Timeline for evaluation (1-2 weeks)\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… LLM Evaluation Framework Prompt:\")\n",
    "print(prompt_llm_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: LLM-as-Judge Prompts\n",
    "\n",
    "Use models to evaluate other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llm_as_judge = \"\"\"\n",
    "You are an expert evaluator assessing chatbot responses.\n",
    "\n",
    "Evaluate the following response on these criteria:\n",
    "\n",
    "1. Correctness (0-10): Is the information factually accurate?\n",
    "2. Helpfulness (0-10): Does it solve the customer's problem?\n",
    "3. Tone (0-10): Is it professional, empathetic, and appropriate?\n",
    "4. Completeness (0-10): Does it address all parts of the question?\n",
    "5. Conciseness (0-10): Is it brief without sacrificing clarity?\n",
    "\n",
    "Customer Question: \"I ordered a blue sweater size M but received a red one size L. What should I do?\"\n",
    "\n",
    "Chatbot Response: \"I apologize for the mix-up! We'll send you a prepaid return label via email within 1 hour. Once we receive the incorrect item, we'll ship the correct blue sweater (size M) with expedited shipping at no charge. You should have it within 3-5 business days. Is there anything else I can help with?\"\n",
    "\n",
    "Provide scores and brief justification for each criterion.\n",
    "\"\"\"\n",
    "\n",
    "print(\"âš–ï¸ LLM-as-Judge Evaluation Prompt:\")\n",
    "print(prompt_llm_as_judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model-Specific Strengths\n",
    "\n",
    "Understanding when to use each model is key to effective prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opus 4.6: The Deep Thinker ðŸ§ \n",
    "\n",
    "**Best For:**\n",
    "- Complex multi-step reasoning\n",
    "- Research-grade analysis\n",
    "- Novel problem-solving\n",
    "- Comprehensive documentation\n",
    "\n",
    "**Prompting Strategy:**\n",
    "- Give it hard problems that benefit from \"thinking time\"\n",
    "- Ask for multiple perspectives or approaches\n",
    "- Request detailed explanations and justifications\n",
    "- Use for one-off deep dives, not high-volume production\n",
    "\n",
    "**Example Use Case:**\n",
    "\"Analyze this novel fraud pattern we've never seen before and propose 3 detection strategies with pros/cons.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonnet 4.6: The Balanced Workhorse âš¡\n",
    "\n",
    "**Best For:**\n",
    "- Production workflows requiring speed + quality\n",
    "- Batch processing tasks\n",
    "- Code generation and review\n",
    "- Technical documentation\n",
    "\n",
    "**Prompting Strategy:**\n",
    "- Emphasize efficiency and clarity\n",
    "- Request structured outputs (JSON, tables, markdown)\n",
    "- Good for iterative refinement\n",
    "- Cost-effective for high-volume use\n",
    "\n",
    "**Example Use Case:**\n",
    "\"Generate 100 synthetic fraud examples following this pattern with varying parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen 2.5: The Open-Source Powerhouse ðŸ”“\n",
    "\n",
    "**Best For:**\n",
    "- On-premise deployments\n",
    "- Privacy-sensitive applications\n",
    "- Multilingual tasks (strong in Chinese and English)\n",
    "- Cost-conscious projects\n",
    "- Fine-tuning for specialized domains\n",
    "\n",
    "**Prompting Strategy:**\n",
    "- Leverage its coding strength with technical prompts\n",
    "- Use for tasks where you can iterate and fine-tune\n",
    "- Strong at following structured formats\n",
    "- Good balance of performance and resource usage\n",
    "\n",
    "**Example Use Case:**\n",
    "\"Generate Python code to transform this raw transaction log into features for a fraud model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-5: The Bilingual Engineer ðŸŒ\n",
    "\n",
    "**Best For:**\n",
    "- Chinese-English bilingual tasks\n",
    "- Technical system design\n",
    "- Code generation with architectural context\n",
    "- Global team collaboration\n",
    "\n",
    "**Prompting Strategy:**\n",
    "- Explicitly request bilingual outputs when needed\n",
    "- Strong at combining architecture + implementation\n",
    "- Good for explaining technical concepts to mixed audiences\n",
    "- Effective at code review and optimization\n",
    "\n",
    "**Example Use Case:**\n",
    "\"Design a microservices architecture for our fraud detection system. Provide documentation in both English and Chinese for our global engineering team.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Selection Decision Tree\n",
    "\n",
    "Use this flowchart to choose the right model:\n",
    "\n",
    "```\n",
    "START\n",
    "â”œâ”€ Need bilingual (EN/ZH) output?\n",
    "â”‚  â””â”€ YES â†’ GLM-5\n",
    "â”‚  â””â”€ NO â†’ Continue\n",
    "â”‚\n",
    "â”œâ”€ Privacy-sensitive / on-premise required?\n",
    "â”‚  â””â”€ YES â†’ Qwen 2.5\n",
    "â”‚  â””â”€ NO â†’ Continue\n",
    "â”‚\n",
    "â”œâ”€ Task complexity level?\n",
    "â”‚  â”œâ”€ Very High (novel problems, deep research)\n",
    "â”‚  â”‚  â””â”€ Opus 4.6\n",
    "â”‚  â”œâ”€ Medium-High (production systems, standard analysis)\n",
    "â”‚  â”‚  â””â”€ Sonnet 4.6\n",
    "â”‚  â””â”€ Medium (code generation, structured tasks)\n",
    "â”‚     â””â”€ Qwen 2.5 or Sonnet 4.6\n",
    "â”‚\n",
    "â””â”€ Volume considerations?\n",
    "   â”œâ”€ High volume (>1000 requests/day)\n",
    "   â”‚  â””â”€ Sonnet 4.6 or Qwen 2.5\n",
    "   â””â”€ Low volume (deep analysis)\n",
    "      â””â”€ Opus 4.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Best Practices Summary\n",
    "\n",
    "### Universal Prompting Principles\n",
    "\n",
    "1. **Be Specific**: Include context, constraints, and success criteria\n",
    "2. **Structure Requests**: Use numbered lists, sections, clear formatting\n",
    "3. **Provide Examples**: Few-shot learning improves quality significantly\n",
    "4. **Request Reasoning**: Ask models to \"think step-by-step\" for complex tasks\n",
    "5. **Iterate**: Refine prompts based on output quality\n",
    "6. **Validate**: Always verify outputs, especially for critical decisions\n",
    "\n",
    "### Data Science Specific Tips\n",
    "\n",
    "1. **Include Schema**: Always provide data structure and sample statistics\n",
    "2. **Define Success Metrics**: What does \"good\" look like?\n",
    "3. **Request Code**: Ask for implementation alongside analysis\n",
    "4. **Consider Edge Cases**: Explicitly mention potential pitfalls\n",
    "5. **Ask for Trade-offs**: Prompt for pros/cons of different approaches\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Cost Monitoring**: Track token usage, choose appropriate models\n",
    "2. **Latency Budget**: Use faster models when response time matters\n",
    "3. **Fallback Strategy**: Have backup models for availability\n",
    "4. **Evaluation**: Set up automated quality checks for outputs\n",
    "5. **Version Control**: Track prompt versions alongside code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps & Resources\n",
    "\n",
    "### Practice Exercises\n",
    "1. Adapt the fraud analysis prompt to your domain\n",
    "2. Design a custom evaluation framework for your use case\n",
    "3. Create a prompt library for your team's common tasks\n",
    "4. Benchmark different models on your specific workflows\n",
    "\n",
    "### Further Reading\n",
    "- [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [OpenAI Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Qwen 2.5 Documentation](https://qwenlm.github.io/)\n",
    "- [GLM-5 Technical Report](https://github.com/THUDM/GLM-4)\n",
    "\n",
    "### Community\n",
    "- Share your prompts and learnings\n",
    "- Contribute to open-source prompt libraries\n",
    "- Join AI/ML communities for best practices\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** The best prompt is one that's tailored to your specific context. Use these examples as starting points and iterate based on your results! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
